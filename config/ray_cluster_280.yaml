# example: https://raw.githubusercontent.com/ray-project/ray/master/doc/source/cluster/kubernetes/configs/ray-cluster.gpu.yaml
apiVersion: ray.io/v1alpha1
kind: RayCluster
metadata:
  name: raycluster-280
  annotations:
    # fault tolerance https://ray-project.github.io/kuberay/guidance/gcs-ft/
    ray.io/ft-enabled: "true" # <- add this annotation enable GCS FT
    #ray.io/external-storage-namespace: "raycluster-storage" # <- optional, to specify the external storage namespace
spec:
  rayVersion: "2.8.0"
  autoscalerOptions:
    # upscalingMode is "Default" or "Aggressive."
    # Conservative: Upscaling is rate-limited; the number of pending worker pods is at most the size of the Ray cluster.
    # Default: Upscaling is not rate-limited.
    # Aggressive: An alias for Default; upscaling is not rate-limited.
    upscalingMode: Default
    # idleTimeoutSeconds is the number of seconds to wait before scaling down a worker pod which is not using Ray resources.
    idleTimeoutSeconds: 60
  enableInTreeAutoscaling: true
  headGroupSpec:
    serviceType: ClusterIP # Options are ClusterIP, NodePort, and LoadBalancer
    rayStartParams:
      dashboard-host: "0.0.0.0"
    template: # Pod template
      spec: # Pod spec
        restartPolicy: Always
        containers:
        # does the head need gpu?
        - name: ray-head
          image: 159.65.30.72:32000/ray:2.8.0
          imagePullPolicy: IfNotPresent
          resources:
            limits:
              cpu: 4
              memory: 8Gi
              #nvidia.com/gpu: 1
            requests:
              cpu: 2
              memory: 6Gi
              #nvidia.com/gpu: 1
          # Keep this preStop hook in each Ray container config.
          lifecycle:
            preStop:
              exec:
                command: ["/bin/sh","-c","ray stop"]
          # ports: # Optional service port overrides
          # - containerPort: 6379
          #   name: gcs
          # - containerPort: 8265
          #   name: dashboard
          # - containerPort: 10001
          #   name: client
          # - containerPort: 8000
          #   name: serve
  workerGroupSpecs:
  # worker group of up to 4 GPU workers
  - groupName: gpu-group
    replicas: 0
    minReplicas: 0
    maxReplicas: 3
    rayStartParams: 
      num-gpus: "1"
    template:
      spec:
        containers:
        - name: ml-work-group
          image: 159.65.30.72:32000/ray:2.8.0-py39-gpu
          imagePullPolicy: IfNotPresent
          resources:
            limits:
              nvidia.com/gpu: 1
              cpu: 4
              memory: 8Gi
            requests:
              nvidia.com/gpu: 1
              cpu: 2
              memory: 6Gi
          lifecycle:
            preStop:
              exec:
                command: ["/bin/sh","-c","ray stop"]
